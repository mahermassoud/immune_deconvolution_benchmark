# Validation with real data
```{r, include=FALSE}
res_validation = new.env()
```

We use datasets which estimate the immune cell proportions using flow cytometry as an additional validation
for our simulation benchmark.  We use the following three validation datasets (see section \@ref(input-data)):
```{r, cache=TRUE}
datasets = list(
  racle=racle,
  hoek=hoek,
  schelker_ovarian=schelker_ovarian
)
```

We use the following cell types, which are available in (some of) the datasets
```{r}
use_cell_types = c("T cell", "T cell CD8+", "T cell CD4+",
                   "Monocyte", "B cell",
                   "Dendritic cell", "NK cell")
```

```{r, echo=FALSE, cache=TRUE, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
# run deconvolution and include TIMER results

## process a deconvolution result
process_result = function(result, method, dataset_name) {
  result %>%
    map_result_to_celltypes(use_cell_types, method) %>%
    as_tibble(rownames="cell_type") %>%
    na.omit() %>%
    gather(sample, estimate, -cell_type) %>%
    mutate(method=method, dataset=dataset_name)
}

timer_cancer_type = list(racle="SKCM", # actually PBMC
                         hoek="SKCM",
                         schelker_ovarian="OV")


# Run the deconvolution...
all_results = foreach(dataset=datasets, dataset_name=names(datasets), .combine=bind_rows) %:%
  foreach(method = config$deconvolution_methods, .combine=bind_rows) %do% {
    tumor = (dataset_name != "hoek") # hoek is PBMC, all others are tumor
    timer_indications = rep(timer_cancer_type[[dataset_name]], ncol(dataset$expr_mat))
    deconvolute(dataset$expr_mat, method, indications=timer_indications, tumor=tumor,
                expected_cell_types = EXPECTED_CELL_TYPES_FACS) %>%
      process_result(method, dataset_name)
}

all_refs = foreach(dataset=datasets, dataset_name=names(datasets), .combine=bind_rows) %do% {
  dataset$ref %>%
    select(sample, cell_type, true_fraction) %>%
    spread(sample, true_fraction) %>%
    map_result_to_celltypes(use_cell_types) %>%
    as_tibble(rownames="cell_type") %>%
    gather(sample, true_fraction, -cell_type) %>%
    mutate(dataset=dataset_name) %>%
    na.omit()
}
```

Here, we combine the predictions with the 'gold standard' reference data.
```{r, cache=TRUE}
all_results_ref = inner_join(all_results, all_refs,
                             by = c("sample" = "sample",
                                    "dataset" = "dataset",
                                    "cell_type" = "cell_type"))

res_validation$all_results = all_results_ref
```

The scores of the methods have different properties and not all of them are directly comparable.
We distinguish between three types of comparisons:

* absolute scores: allow to compare within *and* between samples.
* scores relative to the total immune cell content: allow to compare within a sample
* scores in arbitrary units, that only allow to compare between samples.

```{r, include=FALSE}
abs_methods = config$abs_methods_validation
within_methods = config$methods_within_sample_comparison
```

## Between- and within-sample comparisons
Only works for methods providing an absolute score (`r paste(abs_methods, collapse=", ")`).
All other methods are included for reference.

```{r, fig.width=10, fig.height=16, echo=FALSE, fig.cap="Comparison of absolute predictions for three validation dataset. "}
all_results_ref %>%
  mutate(dataset2 = dataset) %>%
  bind_rows(all_results_ref %>% mutate(dataset2 = "all")) %>%
  filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  ggplot(aes(x=true_fraction, y=estimate)) +
    geom_point(aes(color=cell_type, shape=dataset)) +
    scale_color_manual(values=color_scales$validation) +
    facet_grid(method~dataset2, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "top",
          strip.text.x = element_text(size=9)) +
    background_grid(major="xy") +
    stat_cor()
```

```{r mixing_abs_val, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
slope = all_results_ref %>%
  filter(method %in% abs_methods) %>%
  group_by(cell_type) %>%
  mutate(n=n_distinct(sample)) %>%
  group_by(cell_type, method, n) %>%
  do(get_slope(.)) %>%
  ungroup()

cell_type_method_mat = crossing(abs_methods, use_cell_types)
colnames(cell_type_method_mat) = c("method", "cell_type")
res_validation$slope = slope
```


### Absolute scores
Both EPIC and quanTIseq provide scores that can be interpreted as absolute cell fractions. Both methods account for the different mRNA contents of the different cell types by applying a scaling factor. As the expression values of the single cells we use for the simulation have already been normalized, these scaling factors are not appropriate. While this has no effect on the correlations discussed in the previous section, it is crucial for deriving correct absolute values. For this reason, assessing the absolute quantification performance is fair on the validation datasets only, which provide geniune bulk RNA-seq data.

We use two measures to assess the absolute deviation: (1) the slope of a linear model fitted to the estimated versus known fractions and (2) the root mean square error (RMSE). The slope quickly allows to evaluate whether a method over- or under-predicts a certain cell type while RMSE gives insight about the true deviations.

```{r, fig.width=9, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Comparison of absolute methods. Values indicate the slope of a linear model fitted to predictions vs. true fractions. Values < 1 indicate an under-prediction of the cell type, values > 1 an over-prediction respectively. The error bars have been computed using `confint` on the result of `lm`. "}
slope %>%
  right_join(cell_type_method_mat) %>%
  # na -> 0 workaround, otherwise text label does not show
  ggplot(aes(x=cell_type, y=ifelse(is.na(slope), 1, slope))) +
    geom_crossbar(aes(color=method, ymin=slope, ymax=slope), stat="identity") +
    geom_hline(yintercept=1, col="grey") +
    geom_text(aes(label=ifelse(is.na(slope), "n/a", NA)), angle=0) +
    geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.2) +
    facet_wrap(~method, nrow=1, labeller = label_wrap_gen()) +
    theme(axis.text.x=element_text(angle = 90, vjust = 0.5, hjust=1),
          legend.position="top",
          strip.text.x = element_text(size=9)) +
    scale_fill_manual(values=color_scales$methods, na.value="grey") +
    scale_alpha_manual(values=c("yes"=.3, "no"=1.)) +
    coord_flip(y=c(-5, 10)) +
    ylab("slope of linear fit")
```
```{r abs-rmse-val, fig.width=9, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Comparison of absolute methods. The values show the RMSE"}
rmse = all_results_ref %>%
  filter(method %in% abs_methods) %>%
  mutate(square_error = (estimate-true_fraction)^2) %>%
  group_by(method, cell_type) %>%
  summarise(rmse = sqrt(mean(square_error)))

rmse %>%
 right_join(cell_type_method_mat) %>%
 ggplot(aes(x=cell_type, y=ifelse(is.na(rmse), 0, rmse))) +
    geom_bar(aes(fill=method), stat="identity") +
    geom_text(aes(label=ifelse(is.na(rmse), "n/a", NA)), angle=90, y=0.05) +
    facet_wrap(~method, nrow=1, labeller = label_wrap_gen()) +
    theme(axis.text.x=element_text(angle = 90, vjust = 0.5, hjust=1),
          legend.position="top",
          strip.text.x = element_text(size=9)) +
    scale_fill_manual(values=color_scales$methods, na.value="grey") +
    scale_alpha_manual(values=c("yes"=.3, "no"=1.)) +
    coord_flip() +
    geom_hline(yintercept=0, col="grey") +
    ylab("RMSE")

res_validation$rmse = rmse
```

## Within-sample comparison
only works with methods that provide an absolute score, or a score that is relative to total immune
cell content (`r paste(within_methods, collapse=", ")`).

```{r, fig.width=8, fig.height=12, echo=FALSE, fig.cap="Comparison of predictions within each individual sample"}
all_results_ref %>%
  filter(method %in% within_methods) %>%
  filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  ggplot(aes(x=true_fraction, y=estimate)) +
    geom_point(aes(color=cell_type, shape=dataset)) +
    scale_color_manual(values=color_scales$validation) +
    facet_grid(sample~method, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "right",
          strip.text.x = element_text(size=9),
          strip.text.y = element_text(size=7)) +
    background_grid(major="xy") +
    stat_cor(size=3)
```

Compute the average over all samples:
```{r, fig.width=8, fig.height=3, echo=FALSE, warning=FALSE, fig.cap="Correlations of within-sample comparisons. The last column shows the mean over all samples. "}
sample_correlations = all_results_ref %>%
  filter(method %in% within_methods) %>%
  filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  group_by(dataset, method, sample) %>%
  do(make_cor(.$true_fraction, .$estimate))

average = sample_correlations %>%
  group_by(method) %>%
  summarise(pearson = mean(pearson)) %>%
  mutate(sample = "mean") %>%
  mutate(dataset = "mean")

sample_correlations %>%
  bind_rows(average) %>%
  mutate(pearson_text = if_else(pearson < 0, "< 0", as.character(round(pearson, 2))),
         pearson = if_else(pearson < 0, 0, pearson)) %>%
  ggplot(aes(x=sample, y=method)) +
    geom_tile(aes(fill=pearson)) +
    geom_text(aes(label=pearson_text), size=3) +
    scale_fill_distiller(type="div", palette = "RdYlGn", direction=1, values=c(0,1))  +
    theme(axis.text.x=element_text(angle = 90, vjust = .5, hjust=1))

res_validation$within_sample = average
```


## Between-sample comparisons
For this, we need to look at every cell type independently.
Works for all methods except CIBERSORT.

```{r, fig.width=12, fig.height=12, echo=FALSE, fig.cap="Correlations of known vs. predicted fractions for each cell type independenctly. The 'Tcell' column corresponds to the amount of profiled total T cells or the sum of CD4+ and CD8+ T cells respectively. "}
all_results_ref %>%
  ggplot(aes(x=true_fraction, y=estimate)) +
    geom_point(aes(color=cell_type, shape=dataset)) +
    scale_color_manual(values=color_scales$validation) +
    facet_grid(method~cell_type, scales = "free_y") +
    geom_abline(slope = 1, intercept = 0, color="grey") +
    theme_bw() +
    theme(legend.position = "top",
          strip.text.x = element_text(size=9)) +
    background_grid(major="xy") +
    stat_cor(method='pearson')
```

Compute the average over all cell types.
We only use cell types with at least 5 samples to obtain reasonable
correlation estimates. We also exclude the T-cell supercategory to
avoid redundancies.

```{r}
use_cell_types2 = c("B cell", "Dendritic cell", "Macrophage/Monocyte", "NK cell", "T cell CD4+", "T cell CD8+")
```

```{r, fig.width=5, fig.height=4, echo=FALSE, warning=FALSE, fig.cap="Performance on validation datasets by cell type. The last column shows the mean over all cell types. "}
cell_type_correlations = all_results_ref %>%
#   filter(method %in% between_methods) %>%
  filter(cell_type %in% use_cell_types2) %>%
  group_by(cell_type, method) %>%
  do(make_cor(.$true_fraction, .$estimate))

average = cell_type_correlations %>%
  group_by(method) %>%
  summarise(pearson = mean(pearson)) %>%
  mutate(cell_type = "mean")

cell_type_correlations %>%
  bind_rows(average) %>%
  ungroup() %>%
  mutate(pearson_text = if_else(pearson < 0, "< 0", as.character(round(pearson, 2))),
         pearson = if_else(pearson < 0, 0, pearson)) %>%
  mutate(cell_type = factor(cell_type, levels = c(sort(use_cell_types2), "mean"))) %>%
  ggplot(aes(x=cell_type, y=method)) +
    geom_tile(aes(fill=pearson)) +
    geom_text(aes(label=pearson_text), size=3) +
    scale_fill_distiller(type="div", palette = "RdYlGn", direction=1, values=c(0,1))  +
    theme(axis.text.x=element_text(angle = 90, vjust = .5, hjust=1))

res_validation$between_sample_average = average
res_validation$between_sample = cell_type_correlations
res_validation$use_cell_types = use_cell_types2
```

Note that, although the correlation for CD8+ T cells looks bad, it does not necessarily mean the predictions are bad. There is just little variance between the samples and CD8+ T cell abundance is generally low, which is correlctly predicted by the methods.




